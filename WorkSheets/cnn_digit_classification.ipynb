{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install software\n",
    "Please make sure you only run this cell at the correct stage of the setup. \n",
    "\n",
    "You can run a code cells by bringing it into focus (e.g. clicking in it) and pressing \"ctrl+Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!python -m pip install pandas\n",
    "!python -m pip install -U scikit-learn\n",
    "!python -m pip install -U matplotlib\n",
    "!python -m pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Run the following code cell to download and prepare the MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadTranslateCrop:\n",
    "    def __init__(self, pad, translate_px):\n",
    "        self.pad = pad\n",
    "        self.translate_px = translate_px\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Pad the image\n",
    "        img = transforms.functional.pad(img, self.pad, fill=0)\n",
    "        # Translate the image\n",
    "        dx = torch.randint(-self.translate_px, self.translate_px + 1, (1,)).item()\n",
    "        dy = torch.randint(-self.translate_px, self.translate_px + 1, (1,)).item()\n",
    "        img = transforms.functional.affine(img, angle=0, translate=(dx, dy), scale=1, shear=0, fill=0)\n",
    "        # Crop back to original size\n",
    "        w, h = img.size\n",
    "        left = (w - 28) // 2\n",
    "        top = (h - 28) // 2\n",
    "        img = transforms.functional.crop(img, top, left, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Usage in your transform pipeline\n",
    "custom_transform = transforms.Compose([\n",
    "    PadTranslateCrop(pad=14, translate_px=10),  # pad and allow up to 10px translation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and prepare the training and test datasets\n",
    "data_train = datasets.MNIST(root='./data', train=True, download=True, transform=custom_transform)\n",
    "data_test = datasets.MNIST(root='./data', train=False, download=True, transform=custom_transform)\n",
    "\n",
    "# Set the split sizes\n",
    "train_size = int(0.8 * len(data_train))\n",
    "val_size = len(data_train) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, val_subset = random_split(data_train, [train_size, val_size])\n",
    "\n",
    "### Plot sample of images\n",
    "sample_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "# Get a batch of images and labels from the train_loader\n",
    "images, labels = next(iter(sample_loader))\n",
    "# Plot the first 8 images in the batch\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(labels.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i, 0].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f\"Label: {labels[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters to be modified\n",
    "batchSize = 16 # Number of samples used to update the model parameters each iteration\n",
    "learningRate = 0.001 # Determines how quickly the model learns\n",
    "num_episodes = 10000 # How many times we update the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional neural network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2) # 28 input, 28 output\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, padding=2, stride=2) # 28 input, 14 output\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 14 input, 14 output (+pool -> 7 output)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 7 input, 7 output (+pool -> 3 output)\n",
    "        self.conv5 = nn.Conv2d(128, 10, kernel_size=3, padding=0) # 3 input, 1 output (+pool -> 1 output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x)) # 28 input, 28 output\n",
    "        # x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x)) # 28 input, 14 output\n",
    "        # x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv3(x)) # 14 input, 14 output (+pool -> 7 output)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv4(x)) # 7 input, 7 output (+pool -> 3 output)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv5(x)) # 3 input, 1 output (+pool -> 1 output for each class) \n",
    "        # x = x.squeeze(-1)  # Now shape is (batch_size, number of classes)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output to (batch_size, number of classes)\n",
    "        return x\n",
    "\n",
    "# Create the model and setup the loss and optimiser\n",
    "model = CNN().to(device) # Create model and move to GPU\n",
    "criterion = nn.CrossEntropyLoss() # Loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=learningRate) # Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network to classify digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialise arrays for recording performance\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "sm_train_losses = []\n",
    "sm_val_losses = []\n",
    "sm_train_acc = []\n",
    "sm_val_acc = []\n",
    "nSmooth = 50 # Number of episodes to average over for smoothing\n",
    "\n",
    "# Prepare Dataloaders and iterator to sample from dataset\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batchSize, shuffle=False)\n",
    "# Iterator\n",
    "train_iter = iter(train_loader)\n",
    "val_iter = iter(val_loader)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Get next batch, restart if reaches end of dataset\n",
    "    try:\n",
    "        train_data = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        train_data = next(train_iter)\n",
    "    try:\n",
    "        val_data = next(val_iter)\n",
    "    except StopIteration:\n",
    "        val_iter = iter(val_loader)\n",
    "        val_data = next(val_iter)\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    images, labels = train_data[0].to(device), train_data[1].to(device) # Extract input images and class labels from training set\n",
    "    optimizer.zero_grad() # Reset gradients\n",
    "    outputs = model(images) # Forward pass\n",
    "    outputs = outputs.squeeze(-1)  # compress outputs from nBatch x nClass x 1 to nBatch x nClass\n",
    "    loss = criterion(outputs, labels) # Compute loss\n",
    "    loss.backward() # Backward pass: calculate gradients\n",
    "    optimizer.step() # Update model parameters\n",
    "        \n",
    "    # Calculate classification accuracy\n",
    "    _, predicted = torch.max(outputs.detach(), 1)  # Detach outputs before using for accuracy\n",
    "    correct = (predicted == labels).sum().item() # Sum number of correct predictions\n",
    "    accuracy = correct / labels.size(0) * 100.0 # Calculate percentage accuracy\n",
    "\n",
    "    # Add loss and accuracy to records\n",
    "    train_losses.append(loss.item()) \n",
    "    train_acc.append(accuracy) \n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_images, val_labels = val_data[0].to(device), val_data[1].to(device) # Extract input images and class labels from validation set\n",
    "        val_outputs = model(val_images) # Forward pass\n",
    "        val_outputs = val_outputs.squeeze(-1)  # compress outputs from nBatch x nClass x 1 to nBatch x nClass\n",
    "        val_loss = criterion(val_outputs, val_labels) # Compute validation loss\n",
    "        val_losses.append(val_loss.item())\n",
    "        _, val_predicted = torch.max(val_outputs.detach(), 1) # Detach val_outputs before using for accuracy\n",
    "        val_correct = (val_predicted == val_labels).sum().item()\n",
    "        val_accuracy = val_correct / val_labels.size(0) * 100.0\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "    # Update smooth loss and accuracy curves for plotting\n",
    "    if len(train_losses) > nSmooth:\n",
    "        sm_train_losses.append(np.mean(train_losses[-nSmooth:]))\n",
    "        sm_val_losses.append(np.mean(val_losses[-nSmooth:]))\n",
    "        sm_train_acc.append(np.mean(train_acc[-nSmooth:]))\n",
    "        sm_val_acc.append(np.mean(val_acc[-nSmooth:]))\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}/{num_episodes}:\")\n",
    "        print(f\"    Train Loss: {sm_train_losses[-1]:.4f} | Val Loss: {sm_val_losses[-1]:.4f}\")\n",
    "        print(f\"    Train Accuracy: {sm_train_acc[-1]:.2f} | Val Accuracy: {sm_val_acc[-1]:.2f}\")\n",
    "\n",
    "# Open figure with subplots for plotting loss and accuracy curves\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) \n",
    "# Plot losses\n",
    "ax1.plot(train_losses, label='Train Loss', color=(0.4, 0.7, 1.0)) # Actual train losses\n",
    "ax1.plot(val_losses, label='Val Loss', color=(1.0, 0.7, 0.4)) # Actual validation losses\n",
    "ax1.plot(sm_train_losses, label='Smoothed Train Loss', color=(0., 0.2, 0.6)) # Smoothed train losses\n",
    "ax1.plot(sm_val_losses, label='Smoothed Val Loss', color=(0.6, 0.2, 0.)) # Smoothed validation losses\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "# Plot accuracies\n",
    "ax2.plot(train_acc, label='Train Accuracy', color=(0.4, 0.7, 1.0)) # Actual train accuracies\n",
    "ax2.plot(val_acc, label='Val Accuracy', color=(1.0, 0.7, 0.4)) # Actual validation accuracies\n",
    "ax2.plot(sm_train_acc, label='Smoothed Train Accuracy', color=(0., 0.2, 0.6)) # Smoothed train accuracies\n",
    "ax2.plot(sm_val_acc, label='Smoothed Val Accuracy', color=(0.6, 0.2, 0.)) # Smoothed validation accuracies\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save your trained model\n",
    "torch.save(model.state_dict(), './model_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise how well your Neural Network classifies digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot sample of images\n",
    "sample_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "# Get a batch of images and labels from the train_loader\n",
    "images, labels = next(iter(sample_loader))\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./model_trained'))\n",
    "model.to(device)  # Move model to GPU if available\n",
    "model.eval()\n",
    "\n",
    "# Get neural network predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(device))\n",
    "    outputs = outputs.squeeze(-1)  # compress outputs from nBatch x nClass x 1 to nBatch x nClass\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Get probabilities\n",
    "\n",
    "# Plot 16 samples\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "num_samples = 16  # Number of images to show\n",
    "num_cols = 4     # Images per row\n",
    "num_img_rows = (num_samples + num_cols - 1) // num_cols  # Number of image rows\n",
    "num_total_rows = num_img_rows * 2  # Each image row followed by a bar chart row\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2 * num_total_rows))\n",
    "# Height ratios: image rows = 3, bar chart rows = 1 (one third height)\n",
    "gs = gridspec.GridSpec(num_total_rows, num_cols, height_ratios=[3, 1] * num_img_rows)\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    row_img = (idx // num_cols) * 2      # Odd-numbered row for image\n",
    "    col = idx % num_cols\n",
    "    row_bar = row_img + 1                # Even-numbered row for bar chart\n",
    "\n",
    "    # Image subplot\n",
    "    ax_img = fig.add_subplot(gs[row_img, col])\n",
    "    ax_img.imshow(images[idx, 0].cpu().numpy(), cmap='gray')\n",
    "    ax_img.axis('off')\n",
    "    ax_img.set_title(f\"Label: {labels[idx].item()}\\nPred: {probabilities[idx].argmax().item()}\", fontsize=10)\n",
    "\n",
    "    # Bar chart subplot (same width, half height)\n",
    "    ax_bar = fig.add_subplot(gs[row_bar, col])\n",
    "    ax_bar.bar(np.arange(10), probabilities[idx].cpu().numpy())\n",
    "    ax_bar.set_xticks(np.arange(10))\n",
    "    ax_bar.set_ylim(0, 1)\n",
    "    ax_bar.set_title(\"Class Probabilities\", fontsize=9)\n",
    "    ax_bar.tick_params(axis='x', labelsize=8)\n",
    "    ax_bar.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "    # Optional: Remove y-axis for cleaner look\n",
    "    ax_bar.yaxis.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the learned filters in layers 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 16 filters in the first convolutional layer\n",
    "filters1 = model.conv1.weight.data.cpu().numpy()  # Shape: (16, 1, 5, 5)\n",
    "\n",
    "# Plot the first 16 filters in the first convolutional layer\n",
    "# Each filter is a 5x5 image, and we will plot them in a 4x4 grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "fig.suptitle(\"First Layer Filters (conv1)\", fontsize=14)\n",
    "for i in range(16):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(filters1[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Filter {i+1}', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the video feed and display feedback\n",
    "### Filter video feed\n",
    "```custom_filter()```\n",
    "\n",
    "This filter makes the video feed look more similar to the images used to train the model. It applies several different filters in the following sequence:\n",
    "\n",
    "1. turns the colour image to grayscale\n",
    "2. applies an adaptive threshold, turning light areas black, and dark areas white\n",
    "3. applies a smoothing effect\n",
    "4. applies another adaptive filter to increase contrast\n",
    "5. crops the video feed to capture a central square region\n",
    "6. adjusts the mean brightness and contrast\n",
    "\n",
    "### Plot a bar chart showing the neural network's predictions\n",
    "```dynamic_barchart()```\n",
    "\n",
    "This function creates a barchart, like those shown above, which updates with every frame of the video feed to show the neural network's predictions in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_filter(input_image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Adaptive threshold to remove uneven background\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY_INV, 27, 5\n",
    "    )\n",
    "\n",
    "    # Apply Gaussian blur with sigma=1 (ksize=0 lets OpenCV compute from sigma)\n",
    "    blur = cv2.GaussianBlur(thresh, (0, 0), sigmaX=2)\n",
    "\n",
    "    # Adaptive threshold to remove uneven background\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY_INV, 27, 1\n",
    "    )\n",
    "\n",
    "    # Process frame to be greyscale and same shape as training images\n",
    "    image = cv2.resize(thresh[:,80:560], (28, 28), interpolation=cv2.INTER_AREA)/255.0\n",
    "    output_image = (image - 0.1307) / 0.3081  # Normalize the image as per training data\n",
    "\n",
    "    return output_image\n",
    "\n",
    "def dynamic_barchart(probabilities):\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "    ax.bar(np.arange(10), probabilities)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"Prediction Probabilities\")\n",
    "    ax.set_xlabel(\"Digit\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_xticks(np.arange(10))  # Ensure tick marks for every integer 0-9\n",
    "    plt.tight_layout()\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    bar_img = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    bar_img = cv2.imdecode(bar_img, 1)\n",
    "\n",
    "    return bar_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run webcam through model\n",
    "To stop the webcam, press q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open the first webcam (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Create OpenCV window\n",
    "cv2.namedWindow(\"Webcam\", cv2.WINDOW_NORMAL) \n",
    "# Resize window \n",
    "cv2.resizeWindow(\"Webcam\", 640+480, 480) \n",
    "#########################################\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./model_trained'))\n",
    "model.to(device)  # Move model to GPU if available\n",
    "model.eval()\n",
    "\n",
    "##########################################\n",
    "\n",
    "# Loop to capture frames from the webcam\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if frame is successfully read\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Apply filter to video feed\n",
    "    filtered_image = custom_filter(frame)\n",
    "\n",
    "    # Forward pass through model to get class predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.tensor(np.reshape(filtered_image,(1,1,28,28))).float().to(device))\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1).cpu().numpy().flatten()  # Get probabilities\n",
    "    \n",
    "\n",
    "    ### Build feedback display\n",
    "    # Prepare filtered_image for display\n",
    "    image_disp = ((filtered_image - filtered_image.min()) / (filtered_image.max() - filtered_image.min()) * 255).astype(np.uint8)\n",
    "    image_disp = cv2.cvtColor(cv2.resize(image_disp, (480, 480)), cv2.COLOR_GRAY2BGR)\n",
    "    # Concatenate original and normalised images horizontally\n",
    "    top_row = cv2.hconcat([frame, image_disp])\n",
    "    # Create bar chart\n",
    "    bar_img = dynamic_barchart(probabilities)\n",
    "    # Pad norm_disp to match frame_disp height\n",
    "    pad = (top_row.shape[1] - bar_img.shape[1]) // 2\n",
    "    bar_disp = cv2.copyMakeBorder(bar_img, 0, 0, pad, top_row.shape[1] - bar_img.shape[1] - pad, cv2.BORDER_CONSTANT, value=0)\n",
    "    # Stack vertically\n",
    "    combined = cv2.vconcat([top_row, bar_disp])\n",
    "    # Display the combined image\n",
    "    cv2.imshow('Webcam', combined)\n",
    "\n",
    "    # Check for 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
